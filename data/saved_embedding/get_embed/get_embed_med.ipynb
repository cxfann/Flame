{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n",
    "from torch.utils import data\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Batch\n",
    "from itertools import repeat, product, chain\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree, softmax\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "allowable_features = {\n",
    "    'possible_atomic_num_list' : list(range(1, 119)),\n",
    "    'possible_formal_charge_list' : [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5],\n",
    "    'possible_chirality_list' : [\n",
    "        Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "        Chem.rdchem.ChiralType.CHI_OTHER\n",
    "    ],\n",
    "    'possible_hybridization_list' : [\n",
    "        Chem.rdchem.HybridizationType.S,\n",
    "        Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2, Chem.rdchem.HybridizationType.UNSPECIFIED\n",
    "    ],\n",
    "    'possible_numH_list' : [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'possible_implicit_valence_list' : [0, 1, 2, 3, 4, 5, 6],\n",
    "    'possible_degree_list' : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'possible_bonds' : [\n",
    "        Chem.rdchem.BondType.SINGLE,\n",
    "        Chem.rdchem.BondType.DOUBLE,\n",
    "        Chem.rdchem.BondType.TRIPLE,\n",
    "        Chem.rdchem.BondType.AROMATIC\n",
    "    ],\n",
    "    'possible_bond_dirs' : [ # only for double bond stereo information\n",
    "        Chem.rdchem.BondDir.NONE,\n",
    "        Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "        Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "    ]\n",
    "}\n",
    "\n",
    "num_atom_type = 120 #including the extra mask tokens\n",
    "num_chirality_tag = 3\n",
    "\n",
    "num_bond_type = 6 #including aromatic and self-loop edge, and extra masked tokens\n",
    "num_bond_direction = 3 \n",
    "\n",
    "class GINConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Extension of GIN aggregation to incorporate edge information by concatenation.\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int): dimensionality of embeddings for nodes and edges.\n",
    "        embed_input (bool): whether to embed input or not. \n",
    "        \n",
    "\n",
    "    See https://arxiv.org/abs/1810.00826\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, out_dim, aggr = \"add\", **kwargs):\n",
    "        kwargs.setdefault('aggr', aggr)\n",
    "        self.aggr = aggr\n",
    "        super(GINConv, self).__init__(**kwargs)\n",
    "        #multi-layer perceptron\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, out_dim))\n",
    "        self.edge_embedding1 = torch.nn.Embedding(num_bond_type, emb_dim)\n",
    "        self.edge_embedding2 = torch.nn.Embedding(num_bond_direction, emb_dim)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.edge_embedding1.weight.data)\n",
    "        torch.nn.init.xavier_uniform_(self.edge_embedding2.weight.data)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        #add self loops in the edge space\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes = x.size(0))\n",
    "\n",
    "        #add features corresponding to self-loop edges.\n",
    "        self_loop_attr = torch.zeros(x.size(0), 2)\n",
    "        self_loop_attr[:,0] = 4 #bond type for self-loop edge\n",
    "        self_loop_attr = self_loop_attr.to(edge_attr.device).to(edge_attr.dtype)\n",
    "        edge_attr = torch.cat((edge_attr, self_loop_attr), dim = 0)\n",
    "\n",
    "        edge_embeddings = self.edge_embedding1(edge_attr[:,0]) + self.edge_embedding2(edge_attr[:,1])\n",
    "\n",
    "        # return self.propagate(self.aggr, edge_index, x=x, edge_attr=edge_embeddings)\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_embeddings)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return self.mlp(aggr_out)\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_layer (int): the number of GNN layers\n",
    "        emb_dim (int): dimensionality of embeddings\n",
    "        JK (str): last, concat, max or sum.\n",
    "        max_pool_layer (int): the layer from which we use max pool rather than add pool for neighbor aggregation\n",
    "        drop_ratio (float): dropout rate\n",
    "        gnn_type: gin, gcn, graphsage, gat\n",
    "\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, JK = \"last\", drop_ratio = 0, gnn_type = \"gin\"):\n",
    "        super(GNN, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.x_embedding1 = torch.nn.Embedding(num_atom_type, emb_dim)\n",
    "        self.x_embedding2 = torch.nn.Embedding(num_chirality_tag, emb_dim)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.x_embedding1.weight.data)\n",
    "        torch.nn.init.xavier_uniform_(self.x_embedding2.weight.data)\n",
    "\n",
    "        ###List of MLPs\n",
    "        self.gnns = torch.nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            if gnn_type == \"gin\":\n",
    "                self.gnns.append(GINConv(emb_dim, emb_dim, aggr = \"add\"))\n",
    "            elif gnn_type == \"gcn\":\n",
    "                self.gnns.append(GCNConv(emb_dim))\n",
    "            elif gnn_type == \"gat\":\n",
    "                self.gnns.append(GATConv(emb_dim))\n",
    "            elif gnn_type == \"graphsage\":\n",
    "                self.gnns.append(GraphSAGEConv(emb_dim))\n",
    "\n",
    "        ###List of batchnorms\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
    "\n",
    "    #def forward(self, x, edge_index, edge_attr):\n",
    "    def forward(self, *argv):\n",
    "        if len(argv) == 3:\n",
    "            x, edge_index, edge_attr = argv[0], argv[1], argv[2]\n",
    "        elif len(argv) == 1:\n",
    "            data = argv[0]\n",
    "            x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        else:\n",
    "            raise ValueError(\"unmatched number of arguments.\")\n",
    "\n",
    "        x = self.x_embedding1(x[:,0]) + self.x_embedding2(x[:,1])\n",
    "        h_list = [x]\n",
    "        for layer in range(self.num_layer):\n",
    "            h = self.gnns[layer](h_list[layer], edge_index, edge_attr)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            #h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
    "            if layer == self.num_layer - 1:\n",
    "                #remove relu for the last layer\n",
    "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
    "            h_list.append(h)\n",
    "\n",
    "        ### Different implementations of Jk-concat\n",
    "        if self.JK == \"concat\":\n",
    "            node_representation = torch.cat(h_list, dim = 1)\n",
    "        elif self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"max\":\n",
    "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
    "            node_representation = torch.max(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
    "        elif self.JK == \"sum\":\n",
    "            h_list = [h.unsqueeze_(0) for h in h_list]\n",
    "            node_representation = torch.sum(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
    "\n",
    "        return node_representation\n",
    "\n",
    "    def graph_rep(self, *argv):\n",
    "        node_rep = self.forward(*argv)\n",
    "        batch = torch.zeros(node_rep.size(0), dtype=torch.long)\n",
    "        graph_rep = global_mean_pool(node_rep, batch)\n",
    "        return graph_rep\n",
    "\n",
    "    def from_pretrained(self, model_file):\n",
    "        #self.gnn = GNN(self.num_layer, self.emb_dim, JK = self.JK, drop_ratio = self.drop_ratio)\n",
    "        self.load_state_dict(torch.load(model_file))\n",
    "\n",
    "def mol_to_graph_data_obj_simple(mol):\n",
    "    \"\"\"\n",
    "    Converts rdkit mol object to graph Data object required by the pytorch\n",
    "    geometric package. NB: Uses simplified atom and bond features, and represent\n",
    "    as indices\n",
    "    :param mol: rdkit mol object\n",
    "    :return: graph data object with the attributes: x, edge_index, edge_attr\n",
    "    \"\"\"\n",
    "    # atoms\n",
    "    num_atom_features = 2   # atom type,  chirality tag\n",
    "    atom_features_list = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_feature = [allowable_features['possible_atomic_num_list'].index(\n",
    "            atom.GetAtomicNum())] + [allowable_features[\n",
    "            'possible_chirality_list'].index(atom.GetChiralTag())]\n",
    "        atom_features_list.append(atom_feature)\n",
    "    x = torch.tensor(np.array(atom_features_list), dtype=torch.long)\n",
    "\n",
    "    # bonds\n",
    "    num_bond_features = 2   # bond type, bond direction\n",
    "    if len(mol.GetBonds()) > 0: # mol has bonds\n",
    "        edges_list = []\n",
    "        edge_features_list = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_feature = [allowable_features['possible_bonds'].index(\n",
    "                bond.GetBondType())] + [allowable_features[\n",
    "                                            'possible_bond_dirs'].index(\n",
    "                bond.GetBondDir())]\n",
    "            edges_list.append((i, j))\n",
    "            edge_features_list.append(edge_feature)\n",
    "            edges_list.append((j, i))\n",
    "            edge_features_list.append(edge_feature)\n",
    "\n",
    "        # data.edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
    "        edge_index = torch.tensor(np.array(edges_list).T, dtype=torch.long)\n",
    "\n",
    "        # data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "        edge_attr = torch.tensor(np.array(edge_features_list),\n",
    "                                 dtype=torch.long)\n",
    "    else:   # mol has no bonds\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr = torch.empty((0, num_bond_features), dtype=torch.long)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_path = 'data/data_process/output/mimic-iii/voc_final.pkl'\n",
    "db2smiles_path = 'data/data_process/output/mimic-iii/db2SMILES.pkl'\n",
    "model_path = 'Mole-BERT/model/path'  ##\n",
    "\n",
    "med_embed_path = 'data/save_embedding/med_embed_molebert.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(num_layer=5, emb_dim=300, JK=\"last\", drop_ratio=0, gnn_type=\"gin\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "voc = dill.load(open(voc_path, 'rb'))\n",
    "db2smiles = dill.load(open(db2smiles_path, 'rb'))\n",
    "diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "\n",
    "med_embed = dict()\n",
    "\n",
    "for idx, db_id in med_voc.idx2word.items():\n",
    "    if db_id not in db2smiles:\n",
    "        assert f'{db_id} not in db2smiles'\n",
    "    if len(db2smiles[db_id]) != 1:\n",
    "        assert f'{db_id} has multiple smiles'\n",
    "    smiles = db2smiles[db_id][0]\n",
    "    mol = AllChem.MolFromSmiles(smiles)\n",
    "    data = mol_to_graph_data_obj_simple(mol)\n",
    "    graph_rep = model.graph_rep(data)\n",
    "    med_embed[idx] = graph_rep[0].detach()\n",
    "    \n",
    "dill.dump(med_embed, open(med_embed_path, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
